{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference \n",
    "\n",
    "This notebook allows you to do: \n",
    "- ✅ **Inference**: Inference refers to using a trained model to make predictions on new, unseen data. This is the final step after training and evaluation, where the model is applied to real-world data.  \n",
    "   - ✅ **Define Data**: Define the path to a directory or image which you would like to have predictions for.  \n",
    "   - ✅ **Choose Model**: Choose a trained model which you would like to use for making predictions.  \n",
    "   - ✅ **Make Predictions**: Make predictions on the provided data using the selected model.\n",
    "\n",
    "---  \n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to import external libraries, which simplify the implementation of the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from IPython.display import display\n",
    "from deepEM.Utils import create_text_widget, print_info\n",
    "from src.Inferencer import Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In difference to other deep learning methods, a model needs to be trained for each tomogram you wish to reconstruct. This is the case due to the special nature of the approach: It is \"overfitting\" the model to the reconstructed sample itself. Hence, please make sure to execute `1_Development.ipynb` for each reconstruction. \n",
    "\n",
    "Then, you will not need to provide data for inference. The tomogram will be based on the data provided for training in `1_Development.ipynb`.\n",
    "\n",
    "> *Execute the cell below to visulize a text form to provdide the batch size for infernce.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba0e0cf8bb343f487e3073ffc3525c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='64', description='Batch Size:', layout=Layout(width='1000px'), style=TextStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db74f6283394b3da0cf42b529f8c540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Please set the batch size for inference. Larger batch size can lead to faster computa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_widget = create_text_widget(\"Batch Size:\", 64, \"Please set the batch size for inference. Larger batch size can lead to faster computation but may lead to OOM (out of memory) errors.\")\n",
    "\n",
    "display(*batch_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set the Data Path accoring to your input in the text form above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Use batch size of 64 for inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = int(batch_widget[0].value)\n",
    "print_info(f\"Use batch size of {batch_size} for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Choose Model\n",
    "\n",
    "As mentioned above, due to the special nature of this deep learning approach, we require to retrain the model during inference. \n",
    "Still, we need to retrieve the best set of hyperparameters from our previous model developlent. \n",
    "These parameters were saved together with the model weights during the training.\n",
    "Hence, we are loading the model checkpoint.\n",
    "\n",
    "> *Execute the cell below to visulize a text form to provide the path to a trained model to do inference with.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c9734c7374403fb7811182c96d382a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Path:', layout=Layout(width='1000px'), style=TextStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6963293fb962409c84caa793eccfe97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"<b>Hint:</b> Enter the path to a pretrained model (i.e. logs/synthetic_2025-04-01_08-38-23/Trainin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"\", \"Enter the path to a pretrained model (i.e. logs/synthetic_2025-04-01_08-38-23/TrainingRun/checkpoints) which you'd like to use for inference.\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Make Prediction\n",
    "> *Execute the cell below to generate the tomogram based on the data you specified earlier using the model you defined above. Results will be stored within the provided data folder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Found model checkpoint at logs/synthetic_2025-04-02_06-27-45/TrainingRun/checkpoints/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd/hannah/DeepEM-Tomography/src/Inferencer.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.model_path)\n",
      "/mnt/hdd/hannah/.local/lib/python3.11/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Will save results to ./data/synthetic/results-synthetic_2025-04-02_06-27-45/2025-04-03_08-54-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate Tomogram: 100%|██████████| 1074219/1074219 [14:59<00:00, 1194.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Tomogram was saved to ./data/synthetic/results-synthetic_2025-04-02_06-27-45/2025-04-03_08-54-06/tomogram.tif\n"
     ]
    }
   ],
   "source": [
    "model_path = model_widget[0].value\n",
    "inferencer = Inference(model_path, None, batch_size)\n",
    "inferencer.inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
