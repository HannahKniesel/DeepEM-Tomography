{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development \n",
    "\n",
    "\n",
    "## 2D to 3D  \n",
    "\n",
    "### Primary Focus: Tomographic Reconstruction   \n",
    "### Application: Tomographic Reconstruction of STEM tilt series\n",
    "\n",
    "#### Challenge: Evaluation with missing ground truth    \n",
    "#### Required Labels: None\n",
    "\n",
    "TL;DR ðŸ§¬âœ¨ We use deep learning for tomographic reconstruction of 2D STEM projections, following [1,2]. This approach enables 3D volume reconstruction, revealing detailed cellular structures and relationships not visible in 2D.\n",
    "\n",
    "![Teaser](./images/Teaser.gif)\n",
    "\n",
    "---\n",
    "\n",
    "[1] Kniesel, Hannah, et al. \"Clean implicit 3D structure from noisy 2D STEM images.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n",
    "\n",
    "[2] Mildenhall, Ben, et al. \"Nerf: Representing scenes as neural radiance fields for view synthesis.\" Communications of the ACM 65.1 (2021): 99-106.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "This notebook allows you to do: \n",
    "\n",
    "- âœ… **Model Training**: Model training involves optimizing a neural network on a given dataset to learn meaningful patterns, resulting in a trained model that can be used for Evaluation or Inference.  \n",
    "\n",
    "    - âœ… **Hyperparameter Search**: Before full model training, it is recommended to perform a hyperparameter search. This involves multiple short training runs on different hyperparameter sets to approximate full training performance. The best-performing set is then selected. Expanding the search space or using better approximations can improve results but requires more time and compute resources. Make sure you have enough Lightning AI credits before doing so.  \n",
    "\n",
    "    - âœ… **Model Training + Validation**: Once the best hyperparameters are selected, the model is trained on the dataset while monitoring performance on a validation set. This ensures that the model generalizes well and helps prevent overfitting.  \n",
    "\n",
    "- âœ… **Evaluation**: Evaluation assesses the performance of a trained model. This can be done on the test split of the training dataset (this should usually be done after training the model) or on a completely new dataset to check generalization. To evaluate on the full dataset, enable the \"evaluate on full\" checkbox before running the evaluation cell, only do this if you did not train the model on the same data.  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to import external libraries, which simplify the implementation of the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports from the template \n",
    "from deepEM.Utils import create_text_widget, print_info, find_file, create_checkbox_widget\n",
    "from deepEM.Logger import Logger\n",
    "from deepEM.ModelTuner import ModelTuner\n",
    "\n",
    "# costum implementation\n",
    "from src.ModelTrainer import ModelTrainer\n",
    "\n",
    "\n",
    "# import all required libraries\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Data Acquisition  \n",
    "\n",
    "In the case of tomographic reconstruction we do not have access to ground truth information (the 3D structure of the underlying sample). Still, it is important to verify the applicability of the deep learning method. Hence, the use of synthetic data to prove the correctness of the approach and estimate errors. \n",
    "Luckily, we do not need to generate data from scratch, but we can make use of existing synthetic data [1].\n",
    "\n",
    "*[1] Kniesel, Hannah, et al. \"Clean implicit 3d structure from noisy 2d stem images.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2. Data Anntation\n",
    "\n",
    "The applied deep learning method for tomographic reconstruction is a self-supervised appraoch, which means that we do not need annotated data during training of the neural network. \n",
    "Similarly, as we are working with synthetic data, no annotated data is needed for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.3. Data Preprocessing\n",
    "\n",
    "> **Hint**: Note that this step is only required if you wish do adapt this use case to your own dataset. If you like to use the provided data, this step can be skipped. \n",
    "\n",
    "If you wish to reconstruct your own tomogram using the provided notebook, these are the preprocessing steps you need to do:\n",
    "\n",
    "1. **Data Alignment**: As there are usually very small deviations of the image alignment between different acquistion angles, the data should be aligned before reconstruction. While [ImageJ](https://imagej.net/ij/) provides data alignment using SIFT, we usually recommend to use more sophisticated approaches like the tracking of gold particles for alignment (for example by using [iMOD](https://bio3d.colorado.edu/imod/)), to get the best results.\n",
    "\n",
    "2. **Tilt Axis Correction**: In some cases, the tilt axis can be slightly tilted off the main central vertical axis. This needs to be corrected before applying the reconstruction algorithm.\n",
    "Additionally, we require the tilt axis to be vertical. When a horizontal tilt axis is provided, software like [ImageJ](https://imagej.net/ij/) can be used to rotate the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Data Structuring\n",
    "\n",
    "We require the data to be organized in a single folder containing a series of `.tif` files, which are sorted by their names. \n",
    "\n",
    "The folder further needs to contain a `.rawtlt` file with the tilt angles of the EM, again these angles need to be sorted according to the EM image names. When the data is currently in `.mrc` file format, this can be achieved by using the `mrc2tif` command of the [iMOD](https://bio3d.colorado.edu/imod/) software or by using the [ImageJ](https://imagej.net/ij/) software. \n",
    "\n",
    "Lastly, we require a `metadata.json` with following content:\n",
    "\n",
    "```json \n",
    "{\n",
    "    \"slice_thickness_nm\": 550,\n",
    "    \"pixelsize_nmperpixel\": 1.0,\n",
    "    \"original_px_resolution\": 1000\n",
    "}\n",
    "```\n",
    "\n",
    " - `slice_theickness_nm` is the approximated slice thickness of your sample in [nm]. \n",
    " - `pixelsize_nmperpixel` is the pixelsize of your dataset in [nm/px]. \n",
    " - `original_px_resolution` is the image resolution of a single `.tif` in your tilt series in [px]. \n",
    "\n",
    "\n",
    "You can generate such file, within any text editor of your choice. Add the above lines of content and adapt the parameters based on your data. Save the file as `metadata.json`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Synthetic Data\n",
    "Due to the missing ground truth information on real data, the approach should be first trained and tested on synthetic data. The synthetic data consists of a noisy tilt series, which is used for training and a clean tilt series which is used for evaluation. Additionally, we use the phantom volume (hence the ground truth underlying sample of the synthetic data) for evaluation purposes. The data is originally from [1].\n",
    "\n",
    "An example with a tilt series of five EM images and the corresponding `.rawtlt` and `metadata.json` is shown below: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ noisy-projections\n",
    "    â”œâ”€â”€ image_001.tif\n",
    "    â”œâ”€â”€ image_002.tif\n",
    "    â”œâ”€â”€ image_003.tif\n",
    "    â”œâ”€â”€ image_004.tif\n",
    "    â”œâ”€â”€ image_005.tif\n",
    "    â”œâ”€â”€ metadata.json\n",
    "    â””â”€â”€ tilts.rawtlt\n",
    "â”œâ”€â”€ clean-projections\n",
    "    â”œâ”€â”€ image_001.tif\n",
    "    â”œâ”€â”€ image_002.tif\n",
    "    â”œâ”€â”€ image_003.tif\n",
    "    â”œâ”€â”€ image_004.tif\n",
    "    â”œâ”€â”€ image_005.tif\n",
    "    â”œâ”€â”€ metadata.json\n",
    "    â””â”€â”€ tilts.rawtlt\n",
    "â””â”€â”€ phantom-volume\n",
    "    â””â”€â”€ volume.raw\n",
    "\n",
    "```\n",
    "For details please check the provided data (`data/real`) within this use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data\n",
    "\n",
    "If you wish to generate a tomogram from your own tilt series, you can do so, by training the model on the real data. Please note, that due to the special nature of this use case, you will need to train individual models for each tomogram you wish to generate. \n",
    "The datastructure for the real data is similar to the synthetic data, but without a folder for `clean-projections` and the `phantom-volume`. An example structure can be seen below: \n",
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ noisy-projections\n",
    "    â”œâ”€â”€ image_001.tif\n",
    "    â”œâ”€â”€ image_002.tif\n",
    "    â”œâ”€â”€ image_003.tif\n",
    "    â”œâ”€â”€ image_004.tif\n",
    "    â”œâ”€â”€ image_005.tif\n",
    "    â”œâ”€â”€ metadata.json\n",
    "    â””â”€â”€ tilts.rawtlt\n",
    "\n",
    "```\n",
    "Please note, that the data **needs** to be stored within a folder called `noisy-projections`. The parent folder (in the example above called `data`) can be named arbitrarily. For details please check the provided data (`data/real`) within this use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to show a text form. Within this text form you need to define the path to your training data (i.e. `data/synthetic/` or `data/real/`).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbae7b902c14436f8f019554ab19ba0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='./data/synthetic', description='Data Path:', layout=Layout(width='1000px'), style=TextStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6889e02d6d543bea6a7b6ab46ab053f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Enter the path to your data folder.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_widget = create_text_widget(\"Data Path:\",\"./data/synthetic\",\"Enter the path to your data folder.\")\n",
    "display(*data_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set and check the provided Data Path from the text form above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Data path was set to: ./data/synthetic\n"
     ]
    }
   ],
   "source": [
    "data_path = data_widget[0].value\n",
    "print(f\"[INFO]::Data path was set to: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Setup Logging\n",
    "\n",
    "By executing the cell below, we setup the logging directory for the hyperparameter search, model training and evaluation. \n",
    "The logger creates a folder at `./logs/<datafoldername>-<currentdatetime>/`. \n",
    "\n",
    "For each training run there will be one subfolder within the log directory. Training runs of hyperparameter sweeps are called `Sweep_<idx>`, while the subfolder of the final training run is called `TrainingRun`. During evaluation there will be one more subfolder created called `Evaluate`. \n",
    "\n",
    "Within each subfolder folder there will be logging of: \n",
    "\n",
    "- the used hyperparameters, (`<log-path>/<subfolder>/hyperparameters.json`)\n",
    "- the best performing model checkpoint based on the validation loss (`<log-path>/<subfolder>/checkpoints/best_model.pth`)\n",
    "- the last model checkpoint (`<log-path>/<subfolder>/checkpoints/latest_model.pth`)\n",
    "- visualizations of training and validation curves (`<log-path>/<subfolder>/plots/training_curves.png`)\n",
    "- qualitative visualization of sampled validation images (`<log-path>/<subfolder>/samples/`)\n",
    "- results on test metrics (`<log-path>/<subfolder>/test_results.txt`)\n",
    "- qualitative visualization of sampled test images (`<log-path>/<subfolder>/samples/`)\n",
    "\n",
    "\n",
    "Sample visualizations of this use case include the model input, validation labels, predictions, and a GradCAM overlay. GradCAM can be interpreted as an heat map, giving an intuition about \"where the model looks\" to make its prediction.\n",
    "\n",
    "Visualization of training and validation curves will also be shown after every successful training run within this notebook. They can help to identify possible issues like overfitting during training. For more details, we refer to [this guide](https://www.kaggle.com/code/ryanholbrook/overfitting-and-underfitting).\n",
    "\n",
    "> *Exectue the cell below to setup the logger. **Hint** If you wish to train a new model, you can reexecute this cell, to generate a new log directory - allowing you to now override the previousely trained model.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:19:55,375 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-04-07_10-19-55\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters in deep learning control how a model is trained. Unlike learned model parameters, they are set before training. Hyperparameter tuning explores different configurations by training the model multiple times and selecting the best-performing settings based on validation performance. Since this process is time- and resource-intensive, training runs are often limited in duration or dataset size.\n",
    "\n",
    "Our playground automates hyperparameter search using grid search, testing all possible combinations of selected hyperparameters. The search space is initially defined by deep learning (DL) experts, who also provide explanations so electron microscopy (EM) specialists can refine it as needed. In order to do so, you can adapt the form below. Each sweep hyperparameter should be separated by `,`. Floating point values should be written like `0.1`. \n",
    "Logging estimates the remaining time for individual runs and the full sweep, though early estimates may be inaccurate.\n",
    "\n",
    "While not strictly required, a hyperparameter search should be performed at least once per dataset to ensure optimal model performance. Interrupting the search early may yield suboptimal results. The automatic sweep stores tested configurations and the best-performing parameters in the training data directory under `Sweep_Parameters`, allowing reuse for future training, or to resume the sweep if the kernel got interrupted.\n",
    "\n",
    "\n",
    "> *Execute the cell below to show the form of the hyperparameter search space. **Hint** If you've changed parameters and want to reset them to the defaults, reexecute the cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::Found 94 images within tilt series at ./data/synthetic/noisy-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/noisy-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Use 20% for validation, resulting in 18 projection images.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Use 100% for validation, resulting in 94 projection images.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554500cc2d5346ad88318aea73a3371b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Hyperparameter Sweep</h1>'), HTML(value='<p>During hyperparameter sweeps it canâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hyperparameter search\n",
    "model_trainer = ModelTrainer(data_path, logger)\n",
    "\n",
    "hyperparameter_tuner = ModelTuner(model_trainer, data_path, logger)\n",
    "form = hyperparameter_tuner.create_hyperparameter_widgets()\n",
    "display(form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *If you wish to run a hyperparameter sweep based on the parameters above, please execute the cell below. This should be done at least once per dataset. Note that this can take a while. We recomment to set resize to a smaller value (i.e. 100) during hyperparameter search to save resources.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = None\n",
    "hyperparameter_tuner.update_config(form)\n",
    "best_config = hyperparameter_tuner.tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our automatic hyperparameter tuning is able to find the best performing set of hyperparameters based on the setting shown above. \n",
    "\n",
    "However, there can be scenarios, where additional flexibility is required. Therefore, you are able to change these hyperparameters in the following. \n",
    "\n",
    "> *Execute the cell below to show and possibly adapt the currently chosen hyperparameters. We recomment to set resize to a higher value (i.e. 500) for final training to reduce information loss.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08941c910f4e4b569ddba10531430bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Found best hyperparameters (val_loss = 0.0049) for current dataset (synthetic).<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "form = hyperparameter_tuner.edit_hyperparameters()\n",
    "display(form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can increase or decrease the number of training epochs. An 'epoch' in deep learning is one full pass of the model through all the training data, where the model learns and adjusts to improve its predictions. Higher number of epochs leads to longer training but can further improve model performance.\n",
    "\n",
    "> *Execute the cell below to show and possibly adapt the number of training epochs. Leave as is, if you want to train with the suggestion of the DL expert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9fc624a0754dfe9b06525cac093932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='20', description='Epochs:', layout=Layout(width='1000px'), style=TextStyle(description_width='initâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9278549a5c435292e72702647c6432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Higher number of epochs leads to longer training but can further improve model perforâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_trainer.reduce_epochs = None\n",
    "model_trainer.set_epochs()\n",
    "epochs_widget = create_text_widget(\"Epochs:\",str(model_trainer.num_epochs),\"Higher number of epochs leads to longer training but can further improve model performance.\")\n",
    "display(*epochs_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set the hyperparameters and number of training epochs for your training run, based on the forms above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Will use following hyperparameters for future training: {'learning_rate': 0.0005, 'accum_gradients': 4, 'resize': 100} with number of epochs: 20.\n"
     ]
    }
   ],
   "source": [
    "best_config = hyperparameter_tuner.update_hyperparameters(form)\n",
    "model_trainer.num_epochs = max((1,int(epochs_widget[0].value)))\n",
    "print_info(f\"Will use following hyperparameters for future training: {best_config} with number of epochs: {model_trainer.num_epochs}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training and Validation\n",
    "\n",
    "In this section we train and validate the model based on the provided data and hyperparameters resulting from the previous sweep.\n",
    "\n",
    "Training in deep learning is the process where a model learns patterns from labeled data (the one provided at the top of this notebook) by optimizing its parameters through backpropagation. \n",
    "Validation involves using a separate dataset to evaluate the model's performance during training, ensuring it generalizes well to unseen data.\n",
    "\n",
    "Training and validating a model can take a lot of time (ranging from minutes to hours, days or even weeks) depending on the model, the training procedure and the dataset. Our logging module provides approximate times for training, which you can see below the executed training cell or at the `log.txt` within the current log directory (i.e. `<log-dir>/TrainingRun/`). However, these times can be inaccurate, especially at the beginning of training. \n",
    "\n",
    "### Model Checkpoint\n",
    "In the following you can provide a model checkpoint for training. There are two different scenarios when you might want to provide a model checkpoint:\n",
    "\n",
    "1. You wish to resume training. This means, training will picks up exactly where it left off, including learned patterns and settings. This is useful if training was interrupted and needs to be finished from the last saved state. To do so, you need to provide a model checkpoint in the text form below. You can find the last saved checkpoint inside the runs logging directory (`<log-dir>/TrainingRun/checkpoints/latest_model.pth`).\n",
    "2. If you wish to finetune your model. Fine-tuning starts training from the beginning (epoch 0) but uses a pre-trained model as a starting point, already having knowledge about some previousely learned patterns, to improve its performance on a new task or dataset. You can find the best model checkpoint inside the runs logging directory (`<log-dir>/TrainingRun/checkpoints/best_model.pth`).\n",
    "\n",
    "If you wish to finetune the model, you need to check the checkbox below. If you only provide the path to a directory, it will look for a `best_model.pth` or `latest_model.pth` accordingly, within this directory.\n",
    "\n",
    "\n",
    "If you want to start training from scratch (which is usually the case), you can leave the text form below empty.\n",
    "\n",
    "> *Execute the cell below to show a text form. If you wish to resume training, or do finetuning you need to provide a path to a model checkpoint. Leave it empty for standard training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1b485db38547c19487315e6081652e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Checkpoint Path:', layout=Layout(width='1000px'), style=TextStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc88dc39de1f40bc8e7df99a367ed99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> If you wish to resume an earlier training, or do finetuning, enter the path to the laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08731ca94ee421697159bcd9ba4ae1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Enable Finetuning', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b420cbbdf1cb44be900accfe4ad974de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Check this box to do finetuning based on the provided model checkpoint above. This meâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resume_widget = create_text_widget(\"Model Checkpoint Path:\",\"\",\"If you wish to resume an earlier training, or do finetuning, enter the path to the latest_model.pth or the best_model.pth file here.\")\n",
    "checkbox_widget, description_widget = create_checkbox_widget(\n",
    "    name=\"Enable Finetuning\",\n",
    "    value=False,\n",
    "    description=\"Check this box to do finetuning based on the provided model checkpoint above. This means, the models weights will be used for initializing the model, training will be then done as usual.\"\n",
    ")\n",
    "display(*resume_widget, checkbox_widget, description_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to prepare the model and data for training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:23:10,796 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-04-07_10-19-55/TrainingRun\n",
      "2025-04-07 10:23:10,799 - INFO - Hyperparameters saved to logs/synthetic_2025-04-07_10-19-55/TrainingRun/hyperparameters.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::Found 94 images within tilt series at ./data/synthetic/noisy-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (100, 100). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/noisy-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Use 20% for validation, resulting in 18 projection images.\n",
      "INFO::Resized image resolution from (1000, 1000) to (100, 100). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Use 100% for validation, resulting in 94 projection images.\n",
      "INFO::Resized image resolution from (1000, 1000) to (100, 100). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n"
     ]
    }
   ],
   "source": [
    "resume_training = resume_widget[0].value\n",
    "finetuning = checkbox_widget.value\n",
    "if(resume_training):\n",
    "    if(os.path.isfile(resume_training)):\n",
    "        logger.log_info(f\"Found model checkpoint at {resume_training}.\")\n",
    "\n",
    "    else: \n",
    "        if(not finetuning):\n",
    "            resume_training = find_file(resume_training, \"latest_model.pth\")           \n",
    "        else: \n",
    "            resume_training = find_file(resume_training, \"best_model.pth\")\n",
    "            \n",
    "            \n",
    "        if(resume_training is None):\n",
    "            logger.log_error(f\"Could not find resume path at {resume_widget[0].value}. Will start training from scatch.\")\n",
    "        else: \n",
    "            logger.log_info(f\"Found model checkpoint at {resume_training}.\")\n",
    "\n",
    "else:\n",
    "    resume_training = None\n",
    "logger.init(\"TrainingRun\")\n",
    "model_trainer.resume_from_checkpoint = resume_training\n",
    "model_trainer.finetuning = finetuning\n",
    "model_trainer.prepare(best_config, num_epochs = max((1,int(epochs_widget[0].value))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below if you wish to **train** a model. Note that this can take a while.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation\n",
    "Evaluation in deep learning is the process of evaluating a trained model on a separate, unseen dataset to measure its final performance. It provides an unbiased assessment of the model's ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Choose Model \n",
    "\n",
    "In this section we choose the model for testing. \n",
    "If you leave the `Model Path` empty in the text form below, it will use the last model trained.\n",
    "Otherwise, you can define the path to the models best weights at `<log-path>/TrainingRun/checkpoints/best_model.pth` or by providing a path to a directory, which contains `best_model.pth` (like `<log-path>/TrainingRun/`). This allows you to also test shared models or previousely trained models.\n",
    "\n",
    "> *Execute the cell below to show the text form for selecting a model for testing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad372518d46468cb75de0f7a9118d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Path:', layout=Layout(width='1000px'), style=TextStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c916c99477428ca41c7b8351ce4558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> If you wish to test a specific model, you can here define the path to its checkpoint.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"\",\"If you wish to test a specific model, you can here define the path to its checkpoint. (For example: logs/tem-herpes_2025-02-03_11-42-43/TrainingRun/checkpoints)\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaluate\n",
    "We finally evaluate the provided model on the test set. We investigate following metrics: \n",
    "\n",
    "- **Mean Squared Error (MSE)**: A distance metric to compute the squared difference between the prediction and the label. In this use case this means, we compute the difference between the real EM tilt images with the computed EM images based on the learned reconstruction. If MSE is low, the model is able to correctly represent the underlying sample within the tomogram. The lower the metric, the better. \n",
    "\n",
    "- **MSE phantom**: The first metric is not able to capture generalizability of the model to unknown tilt angles, as it only computes the MSE on the tilt series which was used for training. But as we are using synthetic data, we do have access to the underlying phantom volume (the synthetic sample used to compute the tilt series). We hence compare the learned sample with this phantom volume, again by computing the MSE. \n",
    "\n",
    "\n",
    "#### **Summary**\n",
    "| Metric  | Meaning | Interpretation |\n",
    "|---------|---------|---------|\n",
    "| **MSE**  | Distance between true tilt series and tilt series based on the learned reconstruction. | Lower is better |\n",
    "| **MSE phantom**  | Distance between the phantom volume (the synthetic sample used to compute the tilt series) and the learned sample (tomogram) | Lower is better | \n",
    "\n",
    "Visualizations, at `<log-path>/Evaluate/samples/test_*`, show the real EM tilt images (Ground Truth Micrograph) compared to the computed EM images based on the learned reconstruction (Predicted Micrograph). Additionally, when a phantom volume is provided, the learned tomogram is saved to `<log-path>/Evaluate/samples/tomogram.tif`. This file can be opened using [ImageJ](https://imagej.net/ij/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *If you wish to evaluate a model (recommended), execute the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:23:47,623 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-04-07_10-19-55/Evaluate\n",
      "/mnt/hdd/hannah/DeepEM-Tomography/src/ModelTrainer.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::Found 94 images within tilt series at ./data/synthetic/noisy-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/noisy-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Use 20% for validation, resulting in 18 projection images.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Use 100% for validation, resulting in 94 projection images.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:24:22,018 - WARNING - The pixel size of the dataset at ./data/synthetic (1.0) differs from the pixelsize of the loaded model (3.3). Predictions are likely to be incorrect. Make sure that the model you load for evaluation was trained on the data specified on top of this notebook.\n",
      "2025-04-07 10:24:22,019 - WARNING - The slice thickness in nm of the dataset at ./data/synthetic (550) differs from the slice thickness in nm of the loaded model (800). Predictions are likely to be incorrect. Make sure that the model you load for evaluation was trained on the data specified on top of this notebook.\n",
      "2025-04-07 10:24:22,019 - WARNING - The original pixel resolution of the dataset at ./data/synthetic (1000) differs from the original pixel resolution of the loaded model (2042). Predictions are likely to be incorrect. Make sure that the model you load for evaluation was trained on the data specified on top of this notebook.\n",
      "2025-04-07 10:24:22,039 - INFO - Resumed training from checkpoint: logs/final/real_2025-04-03_12-47-44/TrainingRun/checkpoints/best_model.pth (Validation Loss: 0.0416) | Remaining epochs: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to load metadata from: ./data/synthetic/noisy-projections/metadata.json\n",
      "Pixelsize: 1.0 vs. 3.3\n",
      "Pixelsize: 1000 vs. 2042\n",
      "Slice Thickness: 550 vs. 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd/hannah/DeepEM-Tomography/src/STEM.py:80: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  offset = torch.cuda.FloatTensor(batch_size, beam_samples*2).uniform_(float(-bin_size/2), float(bin_size/2))\n",
      "Evaluate:   1%|          | 2340/367188 [00:10<26:08, 232.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m model_trainer\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minit_directories()\n\u001b[1;32m     33\u001b[0m model_trainer\u001b[38;5;241m.\u001b[39mload_checkpoint(eval_model)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_on_full\u001b[49m\u001b[43m)\u001b[49m      \n",
      "File \u001b[0;32m~/DeepEM-Tomography/src/ModelTrainer.py:353\u001b[0m, in \u001b[0;36mModelTrainer.test\u001b[0;34m(self, evaluate_on_full)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_on_full):\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 353\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_on_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;66;03m# load metadata\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;66;03m# TODO retrieve from checkpoint\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoisy-projections\u001b[39m\u001b[38;5;124m\"\u001b[39m),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/DeepEM-Tomography/deepEM/ModelTrainer.py:536\u001b[0m, in \u001b[0;36mAbstractModelTrainer.test\u001b[0;34m(self, evaluate_on_full)\u001b[0m\n\u001b[1;32m    533\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader)\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluate\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 536\u001b[0m     loss, batch_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;66;03m# Accumulate metrics from each batch\u001b[39;00m\n",
      "File \u001b[0;32m~/DeepEM-Tomography/src/ModelTrainer.py:349\u001b[0m, in \u001b[0;36mModelTrainer.test_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03mPerform one test step.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# Implementation could look like this:\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DeepEM-Tomography/src/ModelTrainer.py:327\u001b[0m, in \u001b[0;36mModelTrainer.val_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    325\u001b[0m densities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_small(samples\u001b[38;5;241m.\u001b[39mcuda())   \n\u001b[1;32m    326\u001b[0m predicted_detections \u001b[38;5;241m=\u001b[39m accumulate_beams(densities, samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeam_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m])        \n\u001b[0;32m--> 327\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mdensity_based_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdensities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_origins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_directions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_ends\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeam_samples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    329\u001b[0m densities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(samples\u001b[38;5;241m.\u001b[39mcuda())\n",
      "File \u001b[0;32m~/DeepEM-Tomography/src/STEM.py:80\u001b[0m, in \u001b[0;36mdensity_based_samples\u001b[0;34m(densities, distances, beam_origins, beam_directions, beam_ends, beam_samples)\u001b[0m\n\u001b[1;32m     78\u001b[0m max_distance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39msum(((beam_origins \u001b[38;5;241m-\u001b[39m beam_ends)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     79\u001b[0m bin_size \u001b[38;5;241m=\u001b[39m max_distance[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m beam_samples \n\u001b[0;32m---> 80\u001b[0m offset \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_samples\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbin_size\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbin_size\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# compute samples based on densities\u001b[39;00m\n\u001b[1;32m     83\u001b[0m distances_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(densities\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu(), batch_size\u001b[38;5;241m*\u001b[39mbeam_samples, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "start_evaluation = False\n",
    "eval_model = model_widget[0].value\n",
    "if(eval_model):\n",
    "    eval_model = Path(eval_model)\n",
    "    if(eval_model.is_dir()):\n",
    "        eval_model = Path(find_file(eval_model, \"best_model.pth\")) \n",
    "    if(not eval_model.is_file()):\n",
    "        logger.log_error(f\"Could not find model at {eval_model}. Make sure to train a model before evaluation.\")\n",
    "        eval_model = None\n",
    "    else: \n",
    "        start_evaluation = True\n",
    "else:\n",
    "    recent_logs = logger.get_most_recent_logs()\n",
    "    eval_model = \"\"\n",
    "    for dataname, log_path in recent_logs.items():\n",
    "        if(dataname == Path(data_path).stem):\n",
    "            eval_model = Path(log_path+\"/TrainingRun/checkpoints/best_model.pth\")\n",
    "            if(not eval_model.is_file()):\n",
    "                logger.log_error(f\"Could not find a trained model at {eval_model}. Make sure you fully train a model first before evaluating.\")\n",
    "            else:\n",
    "                logger.log_info(f\"Found most recent log at {eval_model}\")\n",
    "                start_evaluation = True\n",
    "        else: \n",
    "            continue\n",
    "    if(not start_evaluation):\n",
    "        logger.log_error(\"Could not find a trained model. Make sure you train a model first before evaluating.\")\n",
    "      \n",
    "if(start_evaluation):\n",
    "    evaluate_on_full = False\n",
    "    model_trainer.logger.init(f\"Evaluate\")\n",
    "    model_trainer.logger.init_directories()\n",
    "    model_trainer.load_checkpoint(eval_model)\n",
    "    model_trainer.test(evaluate_on_full)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whats Next?\n",
    "\n",
    "## Not satisfied? \n",
    "If you are not satisfied with the evaluation performance of your model you have multiple options: \n",
    "1. Extend the hyperparameter search by adding more, different hyperparameters - they might work better.\n",
    "2. Extend the hyperparameter search to a larger data subset or longer training. This will lead to more accurate results of the sweep, but it will require more compute time. If you do so, make sure to delete the previous state of your hyperparameter search by deleting (or renaming) the folder `<data-path>/Sweep_Parameters`. \n",
    "3. Run multiple training runs with different custom parameters (you can define them right before the cell for training your model).\n",
    "4. Extend the number of training epochs. Longer training can lead to better performance of the model. To do so, you can again define the number of epochs in the cell right before training the model.\n",
    "5. Check the training and sweep runs loggings and train/val curves - maybe you found an issue with the training itself? Or maybe the used metric does not align well with the visual perception of the validation visualizations quality?\n",
    " \n",
    "## Satisfied?\n",
    "If you are satisfied with the results of your trained or evaluated model, there are multiple things to test next:\n",
    " \n",
    "1. Use the trained model for inference. In this special case of DL for EM, this means that the model is used to generate the 3D tomogram based on the tilt series it has been trained on. To do so, open the `2_Inference.ipynb` and follow the steps provided.\n",
    "3. Additionally, you can share your training code and model weights with other collegues. An easy way on how to do this can be found on our website under [\"Getting Started - 5. Collaboration\"](https://viscom-ulm.github.io/DeepEM/getting-started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"./data/real\"\n",
    "\n",
    "metadata_path = find_file(path, \"metadata.json\")\n",
    "with open(metadata_path, 'r') as file:\n",
    "    metadata = json.load(file)\n",
    "pixelsize = metadata[\"pixelsize_nmperpixel\"]\n",
    "slice_thickness_nm = metadata[\"slice_thickness_nm\"]\n",
    "original_px_resolution = metadata[\"original_px_resolution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2867/391753217.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')  # or 'cuda' if needed\n",
      "/tmp/ipykernel_2867/391753217.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')  # or 'cuda' if needed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from deepEM.Utils import find_file\n",
    "\n",
    "p = \"logs/final/real_2025-04-03_12-47-44/Sweep_5\"\n",
    "# Load the checkpoint\n",
    "checkpoint_path = find_file(p, \"best_model.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')  # or 'cuda' if needed\n",
    "\n",
    "# Make sure 'metadata' exists and is a dict\n",
    "metadata = checkpoint.get('metadata', {})\n",
    "if metadata is None:\n",
    "    metadata = {}\n",
    "\n",
    "# Add your new fields\n",
    "metadata['pixelsize_nmperpixel'] = pixelsize\n",
    "metadata['slice_thickness_nm'] = slice_thickness_nm\n",
    "metadata['original_px_resolution'] = original_px_resolution\n",
    "\n",
    "# Assign back to the checkpoint\n",
    "checkpoint['metadata'] = metadata\n",
    "\n",
    "# Save the updated checkpoint (overwrite or save to new file)\n",
    "torch.save(checkpoint, checkpoint_path)  # or a new path if you want to keep the original\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = find_file(p, \"latest_model.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')  # or 'cuda' if needed\n",
    "\n",
    "# Make sure 'metadata' exists and is a dict\n",
    "metadata = checkpoint.get('metadata', {})\n",
    "if metadata is None:\n",
    "    metadata = {}\n",
    "\n",
    "# Add your new fields\n",
    "metadata['pixelsize_nmperpixel'] = pixelsize\n",
    "metadata['slice_thickness_nm'] = slice_thickness_nm\n",
    "metadata['original_px_resolution'] = original_px_resolution\n",
    "\n",
    "# Assign back to the checkpoint\n",
    "checkpoint['metadata'] = metadata\n",
    "\n",
    "# Save the updated checkpoint (overwrite or save to new file)\n",
    "#torch.save(checkpoint, checkpoint_path)  # or a new path if you want to keep the original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 5,\n",
       " 'lr': 0.0005,\n",
       " 'batch_size': 64,\n",
       " 'pos_enc': 5,\n",
       " 'beam_samples': 64,\n",
       " 'accum_gradients': 4,\n",
       " 'resize': 100,\n",
       " 'data_path': './data/real',\n",
       " 'pixelsize_nmperpixel': 3.3,\n",
       " 'slice_thickness_nm': 800,\n",
       " 'original_px_resolution': 2042}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"metadata\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
