{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development \n",
    "\n",
    "\n",
    "## 2D to 3D  \n",
    "\n",
    "### Primary Focus: Tomographic Reconstruction   \n",
    "### Application: Tomographic Reconstruction of STEM tilt series\n",
    "\n",
    "#### Challenge: Evaluation with missing ground truth    \n",
    "#### Required Labels: None\n",
    "\n",
    "TL;DR ðŸ§¬âœ¨ We use deep learning for tomographic reconstruction of 2D STEM projections, following [1,2]. This approach enables 3D volume reconstruction, revealing detailed cellular structures and relationships not visible in 2D.\n",
    "\n",
    "![Teaser](./images/Teaser.gif)\n",
    "\n",
    "---\n",
    "\n",
    "[1] Kniesel, Hannah, et al. \"Clean implicit 3D structure from noisy 2D STEM images.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n",
    "\n",
    "[2] Mildenhall, Ben, et al. \"Nerf: Representing scenes as neural radiance fields for view synthesis.\" Communications of the ACM 65.1 (2021): 99-106.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "This notebook allows you to do: \n",
    "\n",
    "- âœ… **Model Training**: Model training involves optimizing a neural network on a given dataset to learn meaningful patterns, resulting in a trained model that can be used for Evaluation or Inference.  \n",
    "\n",
    "    - âœ… **Hyperparameter Search**: Before full model training, it is recommended to perform a hyperparameter search. This involves multiple short training runs on different hyperparameter sets to approximate full training performance. The best-performing set is then selected. Expanding the search space or using better approximations can improve results but requires more time and compute resources. Make sure you have enough Lightning AI credits before doing so.  \n",
    "\n",
    "    - âœ… **Model Training + Validation**: Once the best hyperparameters are selected, the model is trained on the dataset while monitoring performance on a validation set. This ensures that the model generalizes well and helps prevent overfitting.  \n",
    "\n",
    "- âœ… **Evaluation**: Evaluation assesses the performance of a trained model. This can be done on the test split of the training dataset (this should usually be done after training the model) or on a completely new dataset to check generalization. To evaluate on the full dataset, enable the \"evaluate on full\" checkbox before running the evaluation cell, only do this if you did not train the model on the same data.  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to import external libraries, which simplify the implementation of the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports from the template \n",
    "from deepEM.Utils import create_text_widget, print_info, find_file, create_checkbox_widget\n",
    "from deepEM.Logger import Logger\n",
    "from deepEM.ModelTuner import ModelTuner\n",
    "\n",
    "# costum implementation\n",
    "from src.ModelTrainer import ModelTrainer\n",
    "\n",
    "\n",
    "# import all required libraries\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Data Acquisition  \n",
    "\n",
    "In the case of tomographic reconstruction we do not have access to ground truth information (the 3D structure of the underlying sample). Still, it is important to verify the applicability of the deep learning method. Hence, the use of synthetic data to prove the correctness of the approach and estimate errors. \n",
    "Luckily, we do not need to generate data from scratch, but we can make use of existing synthetic data [1].\n",
    "\n",
    "*[1] Kniesel, Hannah, et al. \"Clean implicit 3d structure from noisy 2d stem images.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2. Data Anntation\n",
    "\n",
    "The applied deep learning method for tomographic reconstruction is a self-supervised appraoch, which means that we do not need annotated data during training of the neural network. \n",
    "Similarly, as we are working with synthetic data, no annotated data is needed for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.3. Data Preprocessing\n",
    "\n",
    "If you wish to reconstruct your own tomogram using the provided notebook, these are the preprocessing steps you need to do:\n",
    "\n",
    "1. **Data Alignment**: As there are usually very small deviations of the image alignment between different acquistion angles, the data should be aligned before reconstruction. While [ImageJ](https://imagej.net/ij/) provides data alignment using SIFT, we usually recommend to use more sophisticated approaches like the tracking of gold particles for alignment (for example by using [iMOD](https://bio3d.colorado.edu/imod/)), to get the best results.\n",
    "\n",
    "2. **Tilt Axis Correction**: In some cases, the tilt axis can be slightly tilted off the main central vertical axis. This needs to be corrected before applying the reconstruction algorithm.\n",
    "Additionally, we require the tilt axis to be vertical. When a horizontal tilt axis is provided, software like [ImageJ](https://imagej.net/ij/) can be used to rotate the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Data Structuring\n",
    "\n",
    "We require the data to be organized in a single folder containing a series of `.tif` files, which are sorted by their names. \n",
    "\n",
    "The folder further needs to contain a `.rawtlt` file with the tilt angles of the EM. When the data is currently in `.mrc` file format, this can be achieved by using the `mrc2tif` command of the [iMOD](https://bio3d.colorado.edu/imod/) software or by using the [ImageJ](https://imagej.net/ij/) software. \n",
    "\n",
    "Lastly, we require a `metadata.json` with following content:\n",
    "\n",
    "```json \n",
    "{\n",
    "    \"slice_thickness_nm\": 550,\n",
    "    \"pixelsize_nmperpixel\": 1.0,\n",
    "    \"original_px_resolution\": 1000\n",
    "}\n",
    "```\n",
    "\n",
    " - `slice_theickness_nm` is the approximated slice thickness of your sample in [nm]. \n",
    " - `pixelsize_nmperpixel` is the pixelsize of your dataset in [nm/px]. \n",
    " - `original_px_resolution` is the image resolution of a single `.tif` in your tilt series in [px]. \n",
    "\n",
    "\n",
    "You can generate such file, within any text editor of your choice. Add the above lines of content and adapt the parameters based on your data. Save the file as `metadata.json`. \n",
    "\n",
    "### Synthetic Data\n",
    "Due to the missing ground truth information on real data, model development will be done on synthetic data. The synthetic data consists of a noisy tilt series, which is used for training and a clean tilt series which is used for evaluation. Additionally, we use the phantom volume (hence the ground truth underlying sample of the synthetic data) for evaluation purposes.\n",
    "\n",
    "An example with a tilt series of five EM images and the corresponding `.rawtlt` and `metadata.json` is shown below: \n",
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ noisy-projections\n",
    "    â”œâ”€â”€ image_001.tif\n",
    "    â”œâ”€â”€ image_002.tif\n",
    "    â”œâ”€â”€ image_003.tif\n",
    "    â”œâ”€â”€ image_004.tif\n",
    "    â”œâ”€â”€ image_005.tif\n",
    "    â”œâ”€â”€ metadata.json\n",
    "    â””â”€â”€ tilts.rawtlt\n",
    "â”œâ”€â”€ clean-projections\n",
    "    â”œâ”€â”€ image_001.tif\n",
    "    â”œâ”€â”€ image_002.tif\n",
    "    â”œâ”€â”€ image_003.tif\n",
    "    â”œâ”€â”€ image_004.tif\n",
    "    â”œâ”€â”€ image_005.tif\n",
    "    â”œâ”€â”€ metadata.json\n",
    "    â””â”€â”€ tilts.rawtlt\n",
    "â””â”€â”€ phantom-volume\n",
    "    â””â”€â”€ volume.raw\n",
    "\n",
    "```\n",
    "For details please check the provided data within this use case.\n",
    "\n",
    "\n",
    "> *Execute the cell below to show a text form. Within this text form you need to define the path to your training data (i.e. `data/tem-herpes/`).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32af1cce909b42b6adc3880ada514b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='./data/synthetic', description='Data Path:', layout=Layout(width='1000px'), style=TextStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1abbbc04bd43da85ddfb34e93f830c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Enter the path to your data folder.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_widget = create_text_widget(\"Data Path:\",\"./data/synthetic\",\"Enter the path to your data folder.\")\n",
    "display(*data_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set and check the provided Data Path from the text form above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Data path was set to: ./data/synthetic\n"
     ]
    }
   ],
   "source": [
    "data_path = data_widget[0].value\n",
    "print(f\"[INFO]::Data path was set to: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Setup Logging\n",
    "\n",
    "By executing the cell below, we setup the logging directory for the hyperparameter search, model training and evaluation. \n",
    "The logger creates a folder at `./logs/<datafoldername>-<currentdatetime>/`. \n",
    "\n",
    "For each training run there will be one subfolder within the log directory. Training runs of hyperparameter sweeps are called `Sweep_<idx>`, while the subfolder of the final training run is called `TrainingRun`. During evaluation there will be one more subfolder created called `Evaluate`. \n",
    "\n",
    "Within each subfolder folder there will be logging of: \n",
    "\n",
    "- the used hyperparameters, (`<log-path>/<subfolder>/hyperparameters.json`)\n",
    "- the best performing model checkpoint based on the validation loss (`<log-path>/<subfolder>/checkpoints/best_model.pth`)\n",
    "- the last model checkpoint (`<log-path>/<subfolder>/checkpoints/latest_model.pth`)\n",
    "- visualizations of training and validation curves (`<log-path>/<subfolder>/plots/training_curves.png`)\n",
    "- qualitative visualization of sampled validation images (`<log-path>/<subfolder>/samples/`)\n",
    "- results on test metrics (`<log-path>/<subfolder>/test_results.txt`)\n",
    "- qualitative visualization of sampled test images (`<log-path>/<subfolder>/samples/`)\n",
    "\n",
    "\n",
    "Sample visualizations of this use case include the model input, validation labels, predictions, and a GradCAM overlay. GradCAM can be interpreted as an heat map, giving an intuition about \"where the model looks\" to make its prediction.\n",
    "\n",
    "Visualization of training and validation curves will also be shown after every successful training run within this notebook. They can help to identify possible issues like overfitting during training. For more details, we refer to [this guide](https://www.kaggle.com/code/ryanholbrook/overfitting-and-underfitting).\n",
    "\n",
    "> *Exectue the cell below to setup the logger. **Hint** If you wish to train a new model, you can reexecute this cell, to generate a new log directory - allowing you to now override the previousely trained model.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:43:24,026 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_11-43-24\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters in deep learning control how a model is trained. Unlike learned model parameters, they are set before training. Hyperparameter tuning explores different configurations by training the model multiple times and selecting the best-performing settings based on validation performance. Since this process is time- and resource-intensive, training runs are often limited in duration or dataset size.\n",
    "\n",
    "Our playground automates hyperparameter search using grid search, testing all possible combinations of selected hyperparameters. The search space is initially defined by deep learning (DL) experts, who also provide explanations so electron microscopy (EM) specialists can refine it as needed. In order to do so, you can adapt the form below. Each sweep hyperparameter should be separated by `,`. Floating point values should be written like `0.1`. \n",
    "Logging estimates the remaining time for individual runs and the full sweep, though early estimates may be inaccurate.\n",
    "\n",
    "While not strictly required, a hyperparameter search should be performed at least once per dataset to ensure optimal model performance. Interrupting the search early may yield suboptimal results. The automatic sweep stores tested configurations and the best-performing parameters in the training data directory under `Sweep_Parameters`, allowing reuse for future training, or to resume the sweep if the kernel got interrupted.\n",
    "\n",
    "\n",
    "> *Execute the cell below to show the form of the hyperparameter search space. **Hint** If you've changed parameters and want to reset them to the defaults, reexecute the cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::Found 94 images within tilt series at ./data/synthetic/noisy-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/noisy-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Use 20% for validation, resulting in 18 projection images.\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (500, 500). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Use 100% for validation, resulting in 94 projection images.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afbcf47fe9d4511a215ece67a7578d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Hyperparameter Sweep</h1>'), HTML(value='<p>During hyperparameter sweeps it canâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hyperparameter search\n",
    "model_trainer = ModelTrainer(data_path, logger)\n",
    "\n",
    "hyperparameter_tuner = ModelTuner(model_trainer, data_path, logger)\n",
    "form = hyperparameter_tuner.create_hyperparameter_widgets()\n",
    "display(form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *If you wish to run a hyperparameter sweep based on the parameters above, please execute the cell below. This should be done at least once per dataset. Note that this can take a while.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:45:52,845 - INFO - Start hyperparameter sweep...\n",
      "2025-03-28 11:45:52,851 - INFO - Found sweep log with current best parameters: {'learning_rate': 0.0005, 'batch_size': 4, 'resize': 100}\n",
      "2025-03-28 11:45:52,890 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_11-43-24/Sweep_0\n",
      "2025-03-28 11:45:52,891 - INFO - Start Sweep 1 of 6...\n",
      "2025-03-28 11:45:52,893 - INFO - Current hyperparams {'learning_rate': 0.0005, 'batch_size': 4, 'resize': 100}\n",
      "2025-03-28 11:45:52,895 - INFO - Current sweep configuration {'learning_rate': 0.0005, 'batch_size': 4, 'resize': 100} already exists at ./data/synthetic/Sweep_Parameters/best_sweep_parameters.json. Continue to next configuration.\n",
      "2025-03-28 11:45:52,898 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_11-43-24/Sweep_1\n",
      "2025-03-28 11:45:52,898 - INFO - Start Sweep 2 of 6...\n",
      "2025-03-28 11:45:52,899 - INFO - Current hyperparams {'learning_rate': 0.0005, 'batch_size': 8, 'resize': 100}\n",
      "2025-03-28 11:45:52,901 - INFO - Current sweep configuration {'learning_rate': 0.0005, 'batch_size': 8, 'resize': 100} already exists at ./data/synthetic/Sweep_Parameters/best_sweep_parameters.json. Continue to next configuration.\n",
      "2025-03-28 11:45:52,903 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_11-43-24/Sweep_2\n",
      "2025-03-28 11:45:52,904 - INFO - Start Sweep 3 of 6...\n",
      "2025-03-28 11:45:52,905 - INFO - Current hyperparams {'learning_rate': 5e-05, 'batch_size': 4, 'resize': 100}\n",
      "2025-03-28 11:45:52,907 - INFO - Current sweep configuration {'learning_rate': 5e-05, 'batch_size': 4, 'resize': 100} already exists at ./data/synthetic/Sweep_Parameters/best_sweep_parameters.json. Continue to next configuration.\n",
      "2025-03-28 11:45:52,909 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_11-43-24/Sweep_3\n",
      "2025-03-28 11:45:52,910 - INFO - Start Sweep 4 of 6...\n",
      "2025-03-28 11:45:52,911 - INFO - Current hyperparams {'learning_rate': 5e-05, 'batch_size': 8, 'resize': 100}\n",
      "2025-03-28 11:45:52,912 - INFO - Current sweep configuration {'learning_rate': 5e-05, 'batch_size': 8, 'resize': 100} already exists at ./data/synthetic/Sweep_Parameters/best_sweep_parameters.json. Continue to next configuration.\n",
      "2025-03-28 11:45:52,914 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_11-43-24/Sweep_4\n",
      "2025-03-28 11:45:52,915 - INFO - Start Sweep 5 of 6...\n",
      "2025-03-28 11:45:52,916 - INFO - Current hyperparams {'learning_rate': 5e-06, 'batch_size': 4, 'resize': 100}\n",
      "2025-03-28 11:45:52,918 - INFO - Current sweep configuration {'learning_rate': 5e-06, 'batch_size': 4, 'resize': 100} already exists at ./data/synthetic/Sweep_Parameters/best_sweep_parameters.json. Continue to next configuration.\n",
      "2025-03-28 11:45:52,920 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_11-43-24/Sweep_5\n",
      "2025-03-28 11:45:52,921 - INFO - Start Sweep 6 of 6...\n",
      "2025-03-28 11:45:52,921 - INFO - Current hyperparams {'learning_rate': 5e-06, 'batch_size': 8, 'resize': 100}\n",
      "2025-03-28 11:45:52,923 - INFO - Current sweep configuration {'learning_rate': 5e-06, 'batch_size': 8, 'resize': 100} already exists at ./data/synthetic/Sweep_Parameters/best_sweep_parameters.json. Continue to next configuration.\n",
      "2025-03-28 11:45:52,924 - INFO - Best Parameters: {'learning_rate': 0.0005, 'batch_size': 4, 'resize': 100}, Best Loss: 0.006118995020286778, from previous sweep.\n",
      "2025-03-28 11:45:52,925 - INFO - Finished sweep with best validation loss = 0.006118995020286778.\n",
      "2025-03-28 11:45:52,926 - INFO - Will use these hyperparameters: {'learning_rate': 0.0005, 'batch_size': 4, 'resize': 100}\n"
     ]
    }
   ],
   "source": [
    "best_config = None\n",
    "hyperparameter_tuner.update_config(form)\n",
    "best_config = hyperparameter_tuner.tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our automatic hyperparameter tuning is able to find the best performing set of hyperparameters based on the setting shown above. \n",
    "\n",
    "However, there can be scenarios, where additional flexibility is required. Therefore, you are able to change these hyperparameters in the following. \n",
    "\n",
    "> *Execute the cell below to show and possibly adapt the currently chosen hyperparameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc0c9b909dc486cb2c67f4966a67838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Found best hyperparameters (val_loss = 0.0061) for current dataset (synthetic).<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "form = hyperparameter_tuner.edit_hyperparameters()\n",
    "display(form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can increase the number of training epochs. An 'epoch' in deep learning is one full pass of the model through all the training data, where the model learns and adjusts to improve its predictions. Higher number of epochs leads to longer training but can further improve model performance.\n",
    "\n",
    "> *Execute the cell below to show and possibly adapt the number of training epochs. Leave as is, if you want to train with the suggestion of the DL expert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eac0fd23ee4cf894a559dc3463e659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='25', description='Epochs:', layout=Layout(width='1000px'), style=TextStyle(description_width='initâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bf302153b14dde8e56f52ce5658ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Higher number of epochs leads to longer training but can further improve model perforâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_trainer.reduce_epochs = None\n",
    "model_trainer.set_epochs()\n",
    "epochs_widget = create_text_widget(\"Epochs:\",str(model_trainer.num_epochs),\"Higher number of epochs leads to longer training but can further improve model performance.\")\n",
    "display(*epochs_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set the hyperparameters and number of training epochs for your training run, based on the forms above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Will use following hyperparameters for future training: {'learning_rate': 0.0005, 'batch_size': 4, 'resize': 100}\n"
     ]
    }
   ],
   "source": [
    "best_config = hyperparameter_tuner.update_hyperparameters(form)\n",
    "model_trainer.num_epochs = int(epochs_widget[0].value)\n",
    "print_info(f\"Will use following hyperparameters for future training: {best_config} with number of epochs: {model_trainer.num_epochs}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training and Validation\n",
    "\n",
    "In this section we train and validate the model based on the provided data and hyperparameters resulting from the previous sweep.\n",
    "\n",
    "Training in deep learning is the process where a model learns patterns from labeled data (the one provided at the top of this notebook) by optimizing its parameters through backpropagation. \n",
    "Validation involves using a separate dataset to evaluate the model's performance during training, ensuring it generalizes well to unseen data.\n",
    "\n",
    "Training and validating a model can take a lot of time (ranging from minutes to hours, days or even weeks) depending on the model, the training procedure and the dataset. Our logging module provides approximate times for training, which you can see below the executed training cell or at the `log.txt` within the current log directory (i.e. `<log-dir>/TrainingRun/`). However, these times can be inaccurate, especially at the beginning of training. \n",
    "\n",
    "### Model Checkpoint\n",
    "In the following you can provide a model checkpoint for training. There are two different scenarios when you might want to provide a model checkpoint:\n",
    "\n",
    "1. You wish to resume training. This means, training will picks up exactly where it left off, including learned patterns and settings. This is useful if training was interrupted and needs to be finished from the last saved state. To do so, you need to provide a model checkpoint in the text form below. You can find the last saved checkpoint inside the runs logging directory (`<log-dir>/TrainingRun/checkpoints/latest_model.pth`).\n",
    "2. If you wish to finetune your model. Fine-tuning starts training from the beginning (epoch 0) but uses a pre-trained model as a starting point, already having knowledge about some previousely learned patterns, to improve its performance on a new task or dataset. You can find the best model checkpoint inside the runs logging directory (`<log-dir>/TrainingRun/checkpoints/best_model.pth`).\n",
    "\n",
    "If you wish to finetune the model, you need to check the checkbox below. If you only provide the path to a directory, it will look for a `best_model.pth` or `latest_model.pth` accordingly, within this directory.\n",
    "\n",
    "\n",
    "If you want to start training from scratch (which is usually the case), you can leave the text form below empty.\n",
    "\n",
    "> *Execute the cell below to show a text form. If you wish to resume training, or do finetuning you need to provide a path to a model checkpoint. Leave it empty for standard training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a19bc9407947c09b5ae1d2bf1d6ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Checkpoint Path:', layout=Layout(width='1000px'), style=TextStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4c4ad4d3474178bf1e105bd67e5ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> If you wish to resume an earlier training, or do finetuning, enter the path to the laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d265ae5db4624879b5ed7f51ea75bcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Enable Finetuning', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2400e2c310040a6a9bf153fb945299d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Check this box to do finetuning based on the provided model checkpoint above. This meâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resume_widget = create_text_widget(\"Model Checkpoint Path:\",\"\",\"If you wish to resume an earlier training, or do finetuning, enter the path to the latest_model.pth or the best_model.pth file here.\")\n",
    "checkbox_widget, description_widget = create_checkbox_widget(\n",
    "    name=\"Enable Finetuning\",\n",
    "    value=False,\n",
    "    description=\"Check this box to do finetuning based on the provided model checkpoint above. This means, the models weights will be used for initializing the model, training will be then done as usual.\"\n",
    ")\n",
    "display(*resume_widget, checkbox_widget, description_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to prepare the model and data for training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:50:45,510 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_11-43-24/TrainingRun\n",
      "2025-03-28 11:50:45,512 - INFO - Hyperparameters saved to logs/synthetic_2025-03-28_11-43-24/TrainingRun/hyperparameters.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::Found 94 images within tilt series at ./data/synthetic/noisy-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (100, 100). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/noisy-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (100, 100). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Use 20% for validation, resulting in 18 projection images.\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (100, 100). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Use 100% for validation, resulting in 94 projection images.\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (100, 100). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Use 100% for validation, resulting in 94 projection images.\n"
     ]
    }
   ],
   "source": [
    "resume_training = resume_widget[0].value\n",
    "finetuning = checkbox_widget.value\n",
    "if(resume_training):\n",
    "    if(os.path.isfile(resume_training)):\n",
    "        logger.log_info(f\"Found model checkpoint at {resume_training}.\")\n",
    "\n",
    "    else: \n",
    "        if(not finetuning):\n",
    "            resume_training = find_file(resume_training, \"latest_model.pth\")           \n",
    "        else: \n",
    "            resume_training = find_file(resume_training, \"best_model.pth\")\n",
    "            \n",
    "            \n",
    "        if(resume_training is None):\n",
    "            logger.log_error(f\"Could not find resume path at {resume_widget[0].value}. Will start training from scatch.\")\n",
    "        else: \n",
    "            logger.log_info(f\"Found model checkpoint at {resume_training}.\")\n",
    "\n",
    "else:\n",
    "    resume_training = None\n",
    "logger.init(\"TrainingRun\")\n",
    "model_trainer.resume_from_checkpoint = resume_training\n",
    "model_trainer.finetuning = finetuning\n",
    "model_trainer.prepare(best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below if you wish to **train** a model. Note that this can take a while.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Run] | Num Epochs: 25 | Dataset size: 940000:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation\n",
    "Evaluation in deep learning is the process of evaluating a trained model on a separate, unseen dataset to measure its final performance. It provides an unbiased assessment of the model's ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Choose Model \n",
    "\n",
    "In this section we choose the model for testing. \n",
    "If you leave the `Model Path` empty in the text form below, it will use the last model trained.\n",
    "Otherwise, you can define the path to the models best weights at `<log-path>/TrainingRun/checkpoints/best_model.pth` or by providing a path to a directory, which contains `best_model.pth` (like `<log-path>/TrainingRun/`). This allows you to also test shared models or previousely trained models.\n",
    "\n",
    "> *Execute the cell below to show the text form for selecting a model for testing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d2a6ad47fb4423b95453fa73cf2ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Path:', layout=Layout(width='1000px'), style=TextStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6f293c84d44698ade161e3b12578a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> If you wish to test a specific model, you can here define the path to its checkpoint.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"\",\"If you wish to test a specific model, you can here define the path to its checkpoint. (For example: logs/tem-herpes_2025-02-03_11-42-43/TrainingRun/checkpoints)\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaluate\n",
    "We finally evaluate the provided model on the test set. We investigate following metrics: \n",
    "\n",
    "- **Mean Squared Error (MSE)**: A distance metric to compute the squared difference between the prediction and the label. In this use case this means, we compute the difference between the real EM tilt images with the computed EM images based on the learned reconstruction. If MSE is low, the model is able to correctly represent the underlying sample within the tomogram. The lower the metric, the better. \n",
    "\n",
    "- **MSE phantom**: The first metric is not able to capture generalizability of the model to unknown tilt angles, as it only computes the MSE on the tilt series which was used for training. But as we are using synthetic data, we do have access to the underlying phantom volume (the synthetic sample used to compute the tilt series). We hence compare the learned sample with this phantom volume, again by computing the MSE. \n",
    "\n",
    "\n",
    "#### **Summary**\n",
    "| Metric  | Meaning | Interpretation |\n",
    "|---------|---------|---------|\n",
    "| **MSE**  | Distance between true tilt series and tilt series based on the learned reconstruction. | Lower is better |\n",
    "| **MSE phantom**  | Distance between the phantom volume (the synthetic sample used to compute the tilt series) and the learned sample (tomogram) | Lower is better | \n",
    "\n",
    "Visualizations, at `<log-path>/Evaluate/samples/test_*`, show the real EM tilt images (Ground Truth Micrograph) compared to the computed EM images based on the learned reconstruction (Predicted Micrograph). Additionally, the learned tomogram is saved to `<log-path>/Evaluate/samples/tomogram.tif`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *If you wish to evaluate a model (recommended), execute the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:23:19,412 - INFO - Found most recent log at logs/synthetic_2025-03-28_09-57-50/TrainingRun/checkpoints/best_model.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::Found 94 images within tilt series at ./data/synthetic/noisy-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (10, 10). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/noisy-projections/tilt.rawtlt\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (10, 10). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Use 20% for validation, resulting in 18 projection images.\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n",
      "INFO::Resized image resolution from (1000, 1000) to (10, 10). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Use 100% for validation, resulting in 94 projection images.\n",
      "INFO::Found 94 images within tilt series at ./data/synthetic/clean-projections/*.tif.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:23:20,621 - INFO - Loaded model checkpoint for finetuning from: logs/synthetic_2025-03-28_09-57-50/TrainingRun/checkpoints/best_model.pth (Validation Loss: 0.0059)\n",
      "2025-03-28 11:23:20,625 - INFO - Logger initialized. Logs will be saved to: logs/synthetic_2025-03-28_09-57-50/Evaluate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::Resized image resolution from (1000, 1000) to (10, 10). Note that strong downscaling can lead to loss of information.\n",
      "INFO::Applied minmax normalization to tilt series. \n",
      " Current max = 1.0000 | Current min = 0.0000\n",
      "INFO:: tilt angles were loaded from ./data/synthetic/clean-projections/tilt.rawtlt\n",
      "INFO::Use 100% for validation, resulting in 94 projection images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 11:23:30,501 - INFO - Test loss: 0.0058\n",
      "2025-03-28 11:23:30,512 - INFO - MSE: 0.0058\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max loaded data: 255\n",
      "INFO::Original shape of volume (1000, 1000, 1000) was resized to (10, 10, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate Tomogram: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:00<00:00, 791.45it/s]\n",
      "2025-03-28 11:23:32,155 - INFO - MSE phantom: 0.0446\n",
      "2025-03-28 11:23:32,159 - INFO - Evaluation Tomogram was saved to logs/synthetic_2025-03-28_09-57-50/Evaluate/samples/tomogram.tif\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "start_evaluation = False\n",
    "eval_model = model_widget[0].value\n",
    "if(eval_model):\n",
    "    eval_model = Path(eval_model)\n",
    "    if(eval_model.is_dir()):\n",
    "        eval_model = Path(find_file(eval_model, \"best_model.pth\")) \n",
    "    if(not eval_model.is_file()):\n",
    "        logger.log_error(f\"Could not find model at {eval_model}. Make sure to train a model before evaluation.\")\n",
    "        eval_model = None\n",
    "    else: \n",
    "        start_evaluation = True\n",
    "else:\n",
    "    recent_logs = logger.get_most_recent_logs()\n",
    "    eval_model = \"\"\n",
    "    for dataname, log_path in recent_logs.items():\n",
    "        if(dataname == Path(data_path).stem):\n",
    "            eval_model = Path(log_path+\"/TrainingRun/checkpoints/best_model.pth\")\n",
    "            if(not eval_model.is_file()):\n",
    "                logger.log_error(f\"Could not find a trained model at {eval_model}. Make sure you fully train a model first before evaluating.\")\n",
    "            else:\n",
    "                logger.log_info(f\"Found most recent log at {eval_model}\")\n",
    "                start_evaluation = True\n",
    "        else: \n",
    "            continue\n",
    "    if(not start_evaluation):\n",
    "        logger.log_error(\"Could not find a trained model. Make sure you train a model first before evaluating.\")\n",
    "      \n",
    "if(start_evaluation):\n",
    "    evaluate_on_full = False\n",
    "    model_trainer.load_checkpoint(eval_model)\n",
    "    model_trainer.test(evaluate_on_full)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whats Next?\n",
    "\n",
    "## Not satisfied? \n",
    "If you are not satisfied with the evaluation performance of your model you have multiple options: \n",
    "1. Extend the hyperparameter search by adding more, different hyperparameters - they might work better.\n",
    "2. Extend the hyperparameter search to a larger data subset or longer training. This will lead to more accurate results of the sweep, but it will require more compute time. \n",
    "3. Run multiple training runs with different custom parameters (you can define them right before the cell for training your model).\n",
    "4. Extend the number of training epochs. Longer training can lead to better performance of the model. To do so, you can again define the number of epochs in the cell right before training the model.\n",
    "5. Finetune or train the model on your own annotated dataset. To do so, we recomment using CVAT for annotation. A general guide for annotating your data with CVAT can ge found on our [\"Getting Started - 4. Data Annotation\"](https://viscom-ulm.github.io/DeepEM/getting-started.html). A more specific guide for this use case can be found at the top of this notebook. \n",
    "6. Check the training and sweep runs loggings and train/val curves - maybe you found an issue with the training itself?\n",
    " \n",
    "## Satisfied?\n",
    "If you are satisfied with the results of your trained or evaluated model, there are multiple things to test next:\n",
    "\n",
    "1. Check the generalizability of your trained model. To do so, you can evaluate your model on different, annotated datasets. Upload your annotated data and define it as `data_path` at the top of this notebook. Check the top of the notebook for data structuring and annotation formats. Run all cells except for `hyperparameter tuning`  and `training`. Then, check the checkbox \"evaluate on full dataset\" such that all of your uploaded data is considered for evalutation. \n",
    "2. Use the trained model for inference. That means using the model on unseen, unlabeled data for the support of your EM data analysis. To do so, open the `2_Inference.ipynb` and follow the steps provided.\n",
    "3. Share and collaborate with other researchers\n",
    "\n",
    "Additionally, you can share your training code and model weights with other collegues. An easy way on how to do this can be found on our website under [\"Getting Started - 5. Collaboration\"](https://viscom-ulm.github.io/DeepEM/getting-started.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
